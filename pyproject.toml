[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "rabbitllm"
version = "1.0.1"
description = "Run 70B+ LLMs on a single 4GB GPU — no quantization required. Layer-streaming inference for consumer hardware."
readme = "README.md"
license = "MIT"
authors = [
    { name = "Manuel S. Lemos", email = "im@manuelslemos.es" }
]
urls = { Homepage = "https://github.com/ManuelSLemos/RabbitLLM" }
requires-python = ">=3.10"
dependencies = [
    "tqdm",
    "torch>=2.5,<2.6",
    # Qwen3MoeForCausalLM requires transformers 4.51.1 exactly
    "transformers==4.51.1",
    "accelerate>=1.1.0",
    "safetensors>=0.4",
    "huggingface-hub>=0.20",
    "scipy",
    "transformers-stream-generator>=0.0.5",
    "tiktoken>=0.12.0",
    "einops>=0.8.2",
    "sentencepiece>=0.1.99",
    "bitsandbytes>=0.49.2",
]
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]

[project.optional-dependencies]
compression = ["bitsandbytes"]
# ⚠️  FLASH ATTENTION WARNING ⚠️
# Do NOT install this extra directly in cloud notebooks or environments
# without a pre‑compiled wheel. Pip will attempt to build from source,
# which can hang for 15+ minutes. Instead:
#   1. Install a matching wheel manually from:
#      https://github.com/Dao-AILab/flash-attention/releases/tag/v2.7.0
#   2. Then install RabbitLLM without the [flash] extra (flash-attn is already present).
# If you don't need FlashAttention, simply omit this extra entirely.
flash = ["flash-attn>=2.7,<2.8"]
server = []

[tool.hatch.build.targets.wheel]
packages = ["src/rabbitllm"]
include = ["src/rabbitllm/**"]

[tool.uv.extra-build-dependencies]
flash-attn = ["torch"]

[tool.ruff]
line-length = 100
target-version = "py310"

[tool.ruff.lint]
select = ["E", "F", "I"]

[tool.pytest.ini_options]
testpaths = ["tests"]
pythonpath = ["src"]

[dependency-groups]
dev = [
    "ruff>=0.9",
    "pytest>=9.0",
    "pytest-cov>=6.0",
    "mypy>=1.0",
]